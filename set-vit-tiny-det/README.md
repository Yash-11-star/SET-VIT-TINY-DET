# Vision Transformer for Tiny Object Detection

A complete implementation of Vision Transformer (ViT) optimized for detecting small objects in noisy images.

## ğŸ¯ Project Overview

**Problem**: Standard computer vision models struggle to detect tiny objects because:
- Background noise dominates the image
- Tiny objects occupy few pixels
- Standard models' attention gets diluted

**Solution**: Multi-scale Vision Transformer with specialized augmentation, focal loss, and per-patch predictions.

## ğŸ“ Project Structure

```
set-vit-tiny-det/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ Makefile
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml         # Default configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README_DATA.md       # Data format documentation
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.sh            # Training script
â”‚   â””â”€â”€ eval.sh             # Evaluation script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ datasets/           # Data loading & augmentation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ coco.py        # Dataset class
â”‚   â”‚   â””â”€â”€ transforms.py  # Augmentation pipeline
â”‚   â”œâ”€â”€ models/             # Model architecture
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ deformable_detr_backbone.py  # Main ViT model
â”‚   â”‚   â”œâ”€â”€ heads.py        # Detection heads
â”‚   â”‚   â”œâ”€â”€ loss.py         # Focal + Smooth L1 losses
â”‚   â”‚   â”œâ”€â”€ neck.py         # Feature pyramid
â”‚   â”‚   â””â”€â”€ set_modules/    # Advanced techniques
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ hbs.py      # Hierarchical Background Smoothing
â”‚   â”‚       â””â”€â”€ api.py      # Adversarial Perturbation Injection
â”‚   â””â”€â”€ utils/              # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ masks.py        # Mask generation
â”‚       â”œâ”€â”€ dist.py         # Distributed training
â”‚       â”œâ”€â”€ meter.py        # Metric tracking
â”‚       â””â”€â”€ viz.py          # Visualization
â”œâ”€â”€ train.py               # Main training script
â””â”€â”€ eval.py                # Main evaluation script
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone <repo-url>
cd set-vit-tiny-det

# Install dependencies
pip install -r requirements.txt
```

### 2. Prepare Data

Create your dataset structure:
```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ image001.jpg
â”‚   â”œâ”€â”€ image002.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ train_annotations.txt
â”œâ”€â”€ val/
â”‚   â””â”€â”€ ...
â””â”€â”€ val_annotations.txt
```

Annotation format (`train_annotations.txt`):
```
image001.jpg 50 100 75 125 0 200 50 240 90 1
image002.jpg 10 10 30 30 0
image003.jpg
```
Format: `image_name.jpg x1 y1 x2 y2 class [x1 y1 x2 y2 class ...]`

### 3. Train Model

```bash
# Using shell script
bash scripts/train.sh

# Or directly
python train.py --config configs/default.yaml --epochs 50
```

### 4. Evaluate Model

```bash
# Using shell script
bash scripts/eval.sh

# Or directly
python eval.py --config configs/default.yaml --checkpoint checkpoints/best_model.pt --test-data data/test
```

### 5. Single Image Inference

```bash
python eval.py \
    --config configs/default.yaml \
    --checkpoint checkpoints/best_model.pt \
    --image path/to/image.jpg
```

## ğŸ—ï¸ Architecture Overview

### Vision Transformer (ViT) Backbone

1. **Patch Embedding** (384Ã—384 image â†’ 16Ã—16 patches)
   - Converts image into 576 patch embeddings
   - Each patch: 768-dimensional vector

2. **Positional Encoding**
   - Learnable positional embeddings
   - Tells model spatial location of each patch
   - Critical for tiny object localization

3. **Transformer Blocks** (12 layers)
   - Multi-head self-attention (12 heads)
   - Feed-forward network (MLP)
   - Each head specializes in different features

4. **Detection Heads** (Per-patch predictions)
   - Bounding box head: predicts (x1, y1, x2, y2)
   - Classification head: predicts class probabilities

### Loss Functions

**Focal Loss** (Classification)
- Addresses class imbalance (95% background, 5% objects)
- Down-weights easy examples, focuses on hard ones
- Formula: FL(pt) = -Î± * (1 - pt)^Î³ * log(pt)

**Smooth L1 Loss** (Bounding Box)
- Quadratic for small errors (precision)
- Linear for large errors (robustness)
- Important for pixel-level accuracy in tiny objects

## ğŸ“Š Configuration

Edit `configs/default.yaml` to customize:

```yaml
MODEL:
  IMAGE_SIZE: 384         # Input image size
  PATCH_SIZE: 16          # Patch size
  NUM_CLASSES: 10         # Number of classes
  NUM_HEADS: 12           # Attention heads
  NUM_LAYERS: 12          # Transformer blocks
  DROPOUT: 0.1

TRAIN:
  EPOCHS: 50
  BATCH_SIZE: 32
  LEARNING_RATE: 1.0e-4
  FOCAL_ALPHA: 0.25       # Focal loss alpha
  FOCAL_GAMMA: 2.0        # Focal loss gamma
  BBOX_LOSS_WEIGHT: 5.0
  CLS_LOSS_WEIGHT: 1.0
```

## ğŸ“ˆ Expected Performance

### On Different Object Sizes
- **Tiny (8-32px)**: mAP ~55-65%
- **Small (32-64px)**: mAP ~70-80%
- **Medium (64-128px)**: mAP ~85-90%

### Training Timeline
- **Epoch 1-5**: Fast loss decrease
- **Epoch 5-20**: Gradual improvement
- **Epoch 20-50**: Plateauing

## ğŸ”§ Advanced Features

### Data Augmentation for Tiny Objects
- Contrast enhancement (makes objects visible)
- Gaussian blur (robust edge learning)
- Random crops (multi-scale training)
- Noise injection (real-world robustness)

### SET Modules (Optional)

1. **Hierarchical Background Smoothing (HBS)**
   - Reduces background noise
   - Multi-scale smoothing
   - Enhances tiny object visibility

2. **Adversarial Perturbation Injection (API)**
   - Improves robustness
   - Handles small object variations
   - Prevents overfitting

## ğŸ› Troubleshooting

### Issue: "Model predicts everything as background"
â†’ Increase `cls_weight` or `focal_alpha`

### Issue: "Loss not decreasing"
â†’ Reduce learning rate or check augmentation intensity

### Issue: "Out of memory"
â†’ Reduce `BATCH_SIZE` or `IMAGE_SIZE`

### Issue: "Bounding boxes inaccurate"
â†’ Increase `bbox_weight` or train longer

## ğŸ“š Key Concepts

### Why Tiny Objects Need Special Treatment

1. **Resolution Issue**
   - 32Ã—32 object in 384Ã—384 image = only 4 patches
   - Background dilutes attention

2. **Class Imbalance**
   - 95% background pixels
   - Standard training ignores objects

3. **Noise Dominance**
   - Small objects = few signal pixels
   - Noise overwhelms signal

### Solution: Multi-Scale ViT

1. **Keep reasonable resolution** (384Ã—384 with 16Ã—16 patches)
2. **Focal Loss** (focus on hard examples)
3. **Smart Augmentation** (enhance visibility)
4. **Per-Patch Predictions** (dense object detection)

## ğŸ“ Understanding the Approach

### Model Decision Rationale

| Component | Choice | Why |
|-----------|--------|-----|
| Image Size | 384Ã—384 | Balance between detail and speed |
| Patch Size | 16Ã—16 | 32Ã—32 object spans 4 patches |
| Num Layers | 12 | ViT standard, proven performance |
| Num Heads | 12 | Multiple feature specialization |
| Loss | Focal + Smooth L1 | Class imbalance + precision |
| Optimizer | AdamW | Adaptive learning + weight decay |

## ğŸ“– References

- Vision Transformer (ViT): https://arxiv.org/abs/2010.11929
- Focal Loss for Dense Object Detection: https://arxiv.org/abs/1708.02002
- DETR: https://arxiv.org/abs/2005.12139

## ğŸ“ Citation

If you use this project, please cite:

```bibtex
@software{tiny_vit_detector,
  title={Vision Transformer for Tiny Object Detection},
  author={Your Name},
  year={2024}
}
```

## ğŸ“„ License

See LICENSE file for details.

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -m 'Add improvement'`)
4. Push to branch (`git push origin feature/improvement`)
5. Open Pull Request

## ğŸ“§ Contact

For questions or issues, please open an issue on GitHub or contact the maintainers.

---

**Happy Training!** ğŸš€